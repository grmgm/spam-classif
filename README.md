# Spam Classification with Naive Bayes 
## Overview. 


Datasets to be used for experimentation: spam.csv. 

Jupyter notebook to be used as starting point: Naive Bayes Spam.ipynb. 

The dataset contains 5,574 messages tagged according to ham (legitimate) or spam. In this experiment we will learn about text features, how to convert them in matrix
form, and apply the Naive Bayes algorithm.

One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. 
Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them.

The other major advantage it has is its relative simplicity. 
Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known.
It rarely ever overfits the data.
Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle.

## Result and code

Please see code [here](https://github.com/mingge612/Decision-Tree-classifier/blob/main/decisionTree.ipynb) for details.
